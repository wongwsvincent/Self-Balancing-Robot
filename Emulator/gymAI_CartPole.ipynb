{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.core import ObservationWrapper\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from models.agentRandomSearch import AgentRandomSearch\n",
    "from models.agentSarsa import AgentSarsa\n",
    "from models.agentQlearning import AgentQlearning\n",
    "from models.agentQlearning_ER import AgentQlearning_ER\n",
    "\n",
    "# configurations\n",
    "numTargetFrame=1000\n",
    "numSucceed=10 # number of epochs it reaches the target\n",
    "numEpoch=int(1e6)\n",
    "numRecordStep=int(1e3)\n",
    "\n",
    "agent_choice='qlearning_er' # choose from 'random_search', 'sarsa', 'qlearning', 'qlearning_er'\n",
    "alpha=0.05 # for 'sarsa', 'qlearning', 'qlearning_er' only\n",
    "gamma=0.9 # for 'sarsa', 'qlearning', 'qlearning_er' only\n",
    "epsilon_max=1.0 # for 'sarsa', 'qlearning', 'qlearning_er' only\n",
    "epsilon_min=0.3 # for 'sarsa', 'qlearning', 'qlearning_er' only\n",
    "epsilon_halflife=200 # for 'sarsa', 'qlearning', 'qlearning_er' only\n",
    "memory_size=1000000 # for 'qlearning_er' only \n",
    "memory_sampling=10 # for 'qlearning_er' only \n",
    "\n",
    "buckets=[5, 5, 20, 20]\n",
    "# buckets=[1, 10, 20, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes the continuous state space to discrete ones\n",
    "class DiscretizeStateWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, buckets=[1, 1, 6, 12], upper_bounds=[], lower_bounds=[]):\n",
    "        super(ObservationWrapper, self).__init__(env)\n",
    "        self.buckets = buckets\n",
    "        if upper_bounds==[]:\n",
    "#             self.upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50) / 1.]\n",
    "            self.upper_bounds = [env.observation_space.high[0]/2, 4, env.observation_space.high[2]/2, math.radians(180) / 1.]\n",
    "        else:\n",
    "            self.upper_bounds = upper_bounds            \n",
    "        if lower_bounds==[]:\n",
    "#             self.lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50) / 1.]\n",
    "            self.lower_bounds = [env.observation_space.low[0]/2, -4, env.observation_space.low[2]/2, -math.radians(180) / 1.]\n",
    "        else:\n",
    "            self.lower_bounds = lower_bounds\n",
    "    def observation(self, obs):\n",
    "        discretized = list()\n",
    "        for i in range(len(obs)):\n",
    "            scaling = (obs[i] + abs(self.lower_bounds[i])) / (self.upper_bounds[i] - self.lower_bounds[i])\n",
    "            new_obs = int(round((self.buckets[i] - 1) * scaling))\n",
    "            new_obs = min(self.buckets[i] - 1, max(0, new_obs)) # VW: needed?\n",
    "            discretized.append(new_obs)\n",
    "        return tuple(discretized)\n",
    "\n",
    "# # doesn't do anything\n",
    "# class RewardWrapper(gym.RewardWrapper):\n",
    "#     def __init__(self, env):\n",
    "#         super().__init__(env)\n",
    "    \n",
    "#     def reward(self, rew):\n",
    "#         # modify rew\n",
    "#         return rew\n",
    "\n",
    "# # doesn't do anything\n",
    "# class ActionWrapper(gym.ActionWrapper):\n",
    "#     def __init__(self, env):\n",
    "#         super().__init__(env)\n",
    "    \n",
    "#     def action(self, act):\n",
    "#         # modify act\n",
    "#         return act\n",
    "    \n",
    "env = DiscretizeStateWrapper(gym.make('CartPole-v1'),buckets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_state_exist(self, state):\n",
    "    if state3 not in self.q_table:\n",
    "        self.q_table[state]=np.zeros(env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A naive agent that goes to right or left depending on the tilt angle\n",
    "# class AgentBasic(object):\n",
    "#     def get_action(self, states):\n",
    "#         return 0 if states[2] < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent):  \n",
    "    \"\"\"Runs the env for a certain amount of steps with the given parameters. Returns the reward obtained\"\"\"\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(numTargetFrame):\n",
    "        state, reward, done = agent.train(state)   \n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df69893ef74a442abc2b80d525d8b61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Number of success', max=10.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a17f8d181574f08813a6229bb3fbd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1000000.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_list=[]\n",
    "reward_list=[]\n",
    "reward_microlist=[]\n",
    "bestreward = 0\n",
    "succeed = 0\n",
    "\n",
    "if(agent_choice=='random_search'):\n",
    "    agent=AgentRandomSearch(env)\n",
    "elif(agent_choice=='sarsa'):\n",
    "    agent=AgentSarsa(env, alpha=alpha, gamma=gamma, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_halflife=epsilon_halflife)\n",
    "elif(agent_choice=='qlearning'):\n",
    "    agent=AgentQlearning(env, alpha=alpha, gamma=gamma, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_halflife=epsilon_halflife)\n",
    "elif(agent_choice=='qlearning_er'):\n",
    "    agent=AgentQlearning_ER(env, alpha=alpha, gamma=gamma, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_halflife=epsilon_halflife, memory_size=100000, memory_sampling=10)\n",
    "\n",
    "if(agent_choice=='sarsa' or agent_choice=='qlearning' or agent_choice=='qlearning_er'):\n",
    "    agent.train_mode(True)\n",
    "    \n",
    "with tqdm(total=numSucceed, desc='Number of success', position=0) as succeedbar:\n",
    "    for i in tqdm(range(numEpoch), desc='Epoch', position=1):  \n",
    "        reward = run_episode(env, agent)\n",
    "        reward_microlist.append(reward)\n",
    "        if reward > bestreward:\n",
    "            bestreward = reward\n",
    "            # considered solved if the agent lasts for the required number of timesteps\n",
    "        if reward == numTargetFrame:\n",
    "            succeed+=1\n",
    "            succeedbar.update(1)\n",
    "            if(succeed==numSucceed):\n",
    "                break\n",
    "        elif(i%numRecordStep==0):\n",
    "            reward_list.append(np.mean(reward_microlist))\n",
    "            epoch_list.append(i)\n",
    "            reward_microlist=[]\n",
    "        \n",
    "if (succeed==numSucceed):\n",
    "    print(\"Finished running and solution found in epoch # {}! =D \\n\".format(i)) # first epoch starts from label 0\n",
    "elif (succeed!=0):\n",
    "    print(\"Finished running and didn't meet requirement, but solution was found {} times. =) \\n\".format(numSucceed)) # first epoch starts from label 0\n",
    "else:\n",
    "    print(\"Finished running but solution not found. =\\\\ \\n\")\n",
    "print(\"The best reward was {} steps.\".format(bestreward))\n",
    "        \n",
    "print(\"#################################\")\n",
    "print(\"#                               #\")\n",
    "print(\"#        Done training!!        #\")\n",
    "print(\"#                               #\")\n",
    "print(\"#################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_list[1:], reward_list[1:])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.ylim([0,np.amax(reward_list[1:])*1.2])\n",
    "plt.savefig('reward_vs_epoch_{}.png'.format(agent_choice))\n",
    "del epoch_list, reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_episode(env):  \n",
    "    \"\"\" Records the frames of the environment obtained using the given parameters... Returns RGB frames\"\"\"\n",
    "    state = env.reset()\n",
    "    firstframe = env.render(mode='rgb_array')\n",
    "    frames = [firstframe]\n",
    "    \n",
    "    if(agent_choice=='sarsa' or agent_choice=='qlearning' or agent_choice=='qlearning_er'):\n",
    "        agent.train_mode(False)\n",
    "    for _ in range(numTargetFrame):\n",
    "        action = agent.get_action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        frames.append(frame)\n",
    "        if done:\n",
    "            break\n",
    "    return frames\n",
    "\n",
    "def display_frames_as_gif(frames, filename_gif = None):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 36)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50, repeat=False)\n",
    "    if filename_gif: \n",
    "        print(\"Saving animation...\")\n",
    "        anim.save(filename_gif, writer = 'pillow', fps=10)\n",
    "        print(\"Animation saves as gif at: {}\".format(filename_gif))\n",
    "        \n",
    "frames = show_episode(env)\n",
    "display_frames_as_gif(frames, filename_gif=\"random_search_play_{}.gif\".format(agent_choice))\n",
    "env.close()\n",
    "print(\"###############################\")\n",
    "print(\"#                             #\")\n",
    "print(\"#        Done saving!!        #\")\n",
    "print(\"#                             #\")\n",
    "print(\"###############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
