{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.core import ObservationWrapper\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "from models.agentRandomSearch import AgentRandomSearch\n",
    "from models.agentSarsa import AgentSarsa\n",
    "\n",
    "# configurations\n",
    "numEpoch=int(1e6)\n",
    "numTargetFrame=200\n",
    "numRecordStep=int(1e2)\n",
    "numSucceed=3 # number of epochs it reaches the target\n",
    "agent_choice='sarsa' # choose from 'random_search', 'sarsa', 'qlearning'\n",
    "alpha=0.1\n",
    "gamma=0.99\n",
    "\n",
    "buckets=[2, 2, 8, 4]\n",
    "# buckets=[1, 10, 10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes the continuous state space to discrete ones\n",
    "class DiscretizeStateWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, buckets=[1, 1, 6, 12], upper_bounds=[], lower_bounds=[]):\n",
    "        super(ObservationWrapper, self).__init__(env)\n",
    "        self.buckets = buckets\n",
    "        if upper_bounds==[]:\n",
    "            self.upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50) / 1.]\n",
    "        else:\n",
    "            self.upper_bounds = upper_bounds            \n",
    "        if lower_bounds==[]:\n",
    "            self.lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50) / 1.]\n",
    "        else:\n",
    "            self.lower_bounds = lower_bounds\n",
    "    def observation(self, obs):\n",
    "        discretized = list()\n",
    "        for i in range(len(obs)):\n",
    "            scaling = (obs[i] + abs(self.lower_bounds[i])) / (self.upper_bounds[i] - self.lower_bounds[i])\n",
    "            new_obs = int(round((self.buckets[i] - 1) * scaling))\n",
    "            new_obs = min(self.buckets[i] - 1, max(0, new_obs)) # VW: needed?\n",
    "            discretized.append(new_obs)\n",
    "        return tuple(discretized)\n",
    "\n",
    "# # doesn't do anything\n",
    "# class RewardWrapper(gym.RewardWrapper):\n",
    "#     def __init__(self, env):\n",
    "#         super().__init__(env)\n",
    "    \n",
    "#     def reward(self, rew):\n",
    "#         # modify rew\n",
    "#         return rew\n",
    "\n",
    "# # doesn't do anything\n",
    "# class ActionWrapper(gym.ActionWrapper):\n",
    "#     def __init__(self, env):\n",
    "#         super().__init__(env)\n",
    "    \n",
    "#     def action(self, act):\n",
    "#         # modify act\n",
    "#         return act\n",
    "    \n",
    "env = DiscretizeStateWrapper(gym.make('CartPole-v1'),buckets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_state_exist(self, state):\n",
    "    if state3 not in self.q_table:\n",
    "        self.q_table[state]=np.zeros(env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A naive agent that goes to right or left depending on the tilt angle\n",
    "# class AgentBasic(object):\n",
    "#     def get_action(self, states):\n",
    "#         return 0 if states[2] < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent):  \n",
    "    \"\"\"Runs the env for a certain amount of steps with the given parameters. Returns the reward obtained\"\"\"\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(numTargetFrame):\n",
    "        state, reward, done = agent.train(state)   \n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch # 0...\n",
      "Running epoch # 1...\n",
      "Running epoch # 2...\n",
      "Running epoch # 3...\n",
      "Running epoch # 4...\n",
      "Running epoch # 5...\n",
      "Running epoch # 6...\n",
      "Running epoch # 7...\n",
      "Running epoch # 8...\n",
      "Running epoch # 9...\n",
      "Running epoch # 10...\n",
      "Running epoch # 20...\n",
      "Running epoch # 30...\n",
      "Running epoch # 40...\n",
      "Running epoch # 50...\n",
      "Running epoch # 60...\n",
      "Running epoch # 70...\n",
      "Running epoch # 80...\n",
      "Running epoch # 90...\n",
      "Running epoch # 100...\n",
      "Running epoch # 200...\n",
      "Running epoch # 300...\n",
      "Running epoch # 400...\n",
      "Running epoch # 500...\n",
      "Running epoch # 600...\n",
      "Running epoch # 700...\n",
      "Running epoch # 800...\n",
      "Running epoch # 900...\n",
      "Running epoch # 1000...\n",
      "Running epoch # 2000...\n",
      "Running epoch # 3000...\n",
      "Running epoch # 4000...\n",
      "Running epoch # 5000...\n",
      "Running epoch # 6000...\n",
      "Running epoch # 7000...\n",
      "Running epoch # 8000...\n",
      "Running epoch # 9000...\n",
      "Running epoch # 10000...\n",
      "Running epoch # 11000...\n",
      "Running epoch # 12000...\n",
      "Running epoch # 13000...\n",
      "Running epoch # 14000...\n",
      "Running epoch # 15000...\n",
      "Running epoch # 16000...\n",
      "Running epoch # 17000...\n",
      "Running epoch # 18000...\n",
      "Running epoch # 19000...\n",
      "Running epoch # 20000...\n",
      "Running epoch # 21000...\n"
     ]
    }
   ],
   "source": [
    "epoch_list=[]\n",
    "reward_list=[]\n",
    "bestreward = 0\n",
    "succeed = 0\n",
    "\n",
    "if(agent_choice=='random_search'):\n",
    "    agent=AgentRandomSearch(env)\n",
    "elif(agent_choice=='sarsa'):\n",
    "    agent=AgentSarsa(env, alpha=alpha, gamma=gamma)\n",
    "elif(agent_choice=='qlearning'):\n",
    "    agent=AgentQlearning(env, alpha=alpha, gamma=gamma)\n",
    "\n",
    "for i in range(numEpoch):  \n",
    "    # varying epsilon to reduce exploration as timestep increases\n",
    "    agent.set_epsilon(math.exp(-numEpoch/100000)) # 0.37 when epoch=100000\n",
    "\n",
    "    reward = run_episode(env, agent)\n",
    "    if reward > bestreward:\n",
    "        bestreward = reward\n",
    "        # considered solved if the agent lasts for the required number of timesteps\n",
    "        if reward == numTargetFrame:\n",
    "            succeed+=1\n",
    "            if(succeed==numSucceed):\n",
    "                reward_list.append(reward)\n",
    "                epoch_list.append(i)\n",
    "                break\n",
    "\n",
    "    if(i<10 or (i<100 and i%10==0) or (i<1000 and i%100==0) or (i%1000==0)):\n",
    "        print(\"Running epoch # {}...\".format(i))\n",
    "    if(i%numRecordStep==0):\n",
    "        reward_list.append(reward)\n",
    "        epoch_list.append(i)\n",
    "\n",
    "if (succeed==numSucceed):\n",
    "    print(\"Finished running and solution found in epoch # {}! =D \\n\".format(i)) # first epoch starts from label 0\n",
    "else:\n",
    "    print(\"Finished running but solution not found. =\\\\ \\n\")\n",
    "print(\"The best reward was {} steps.\".format(bestreward))\n",
    "        \n",
    "print(\"#################################\")\n",
    "print(\"#                               #\")\n",
    "print(\"#        Done training!!        #\")\n",
    "print(\"#                               #\")\n",
    "print(\"#################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_list, reward_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.ylim([0,numTargetFrame*1.2])\n",
    "plt.savefig('reward_vs_epoch.png')\n",
    "del epoch_list, reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_episode(env):  \n",
    "    \"\"\" Records the frames of the environment obtained using the given parameters... Returns RGB frames\"\"\"\n",
    "    state = env.reset()\n",
    "    firstframe = env.render(mode='rgb_array')\n",
    "    frames = [firstframe]\n",
    "    \n",
    "    for _ in range(numTargetFrame):\n",
    "        action = agent.get_action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        frames.append(frame)\n",
    "        if done:\n",
    "            break\n",
    "    return frames\n",
    "\n",
    "def display_frames_as_gif(frames, filename_gif = None):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 36)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50, repeat=False)\n",
    "    if filename_gif: \n",
    "        print(\"Saving animation...\")\n",
    "        anim.save(filename_gif, writer = 'pillow', fps=10)\n",
    "        print(\"Animation saves as gif at: {}\".format(filename_gif))\n",
    "        \n",
    "frames = show_episode(env)\n",
    "display_frames_as_gif(frames, filename_gif=\"random_search_play.gif\")\n",
    "env.close()\n",
    "print(\"###############################\")\n",
    "print(\"#                             #\")\n",
    "print(\"#        Done saving!!        #\")\n",
    "print(\"#                             #\")\n",
    "print(\"###############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
