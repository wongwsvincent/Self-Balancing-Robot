{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# configurations\n",
    "numEpoch=10000\n",
    "numTargetFrame=400\n",
    "# agent='basic'\n",
    "# agent='random_search'\n",
    "\n",
    "buckets=[1, 1, 6, 12]\n",
    "upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50) / 1.]\n",
    "lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50) / 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A naive agent that goes to right or left depending on the tilt angle\n",
    "class AgentBasic(object):\n",
    "    def get_action(self, states):\n",
    "        return 0 if states[2] < 0 else 1\n",
    "\n",
    "# An agent that uses random search\n",
    "class AgentRandomSearch(object):\n",
    "    def __init__(self, env, parameters=None):\n",
    "        self.env = env\n",
    "        if parameters==None:\n",
    "            self.parameters=np.random.rand(4) * 2 - 1\n",
    "        else:\n",
    "            self.parameters=parameters\n",
    "    def get_params(self):\n",
    "        return self.parameters\n",
    "    def set_params(self, parameters):\n",
    "        self.parameters=parameters\n",
    "    def get_action(self, states):\n",
    "        return 0 if np.matmul(self.parameters, states) < 0 else 1\n",
    "    def train(self, state):\n",
    "        action = self.get_action(state)\n",
    "        state_next, reward, terminal, info= self.env.step(action)\n",
    "        return state_next, reward, terminal\n",
    "    \n",
    "class AgentSarsa(object):\n",
    "    def __init__(self, env, state_space, action_space, alpha=0.01, gamma=0.9, epsilon=0.9):\n",
    "        super().__init__(parameters)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.zeros((state_space.n, action_space.n)) \n",
    "        self.actions = action_space\n",
    "        self.env = env\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            target_actions = self.q_table.loc[state, :]\n",
    "            target_actions = target_actions.reindex(np.random.permutation(target_actions.index))\n",
    "            target_action = target_actions.idxmax()\n",
    "        else:\n",
    "            target_action = self.actions.sample()\n",
    "        return target_action\n",
    "\n",
    "    def update_q_table(self, state, action, reward, state_next, action_next, terminal):\n",
    "        q_value_predict = self.q_table.loc[state, action]\n",
    "        if terminal == False:\n",
    "            q_value_real = reward + self.gamma * self.q_table.loc[state_next, action_next]\n",
    "        else:\n",
    "            q_value_real = reward\n",
    "        self.q_table.loc[state, action] += self.alpha * (q_value_real - q_value_predict)\n",
    "\n",
    "    def train(self, state):\n",
    "        # Get first action.\n",
    "        action = self.get_action(state)\n",
    "        # Get next state.\n",
    "        state_next, reward, terminal, info = self.env.step(action)\n",
    "        # Get next action.\n",
    "        action_next = self.get_action(state_next)\n",
    "        # Update Q table.\n",
    "        self.update_q_table(state, action, reward, state_next, action_next, terminal)\n",
    "        return state_next, reward, terminal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(env, obs):\n",
    "    discretized = list()\n",
    "    for i in range(len(obs)):\n",
    "        scaling = (obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i])\n",
    "        new_obs = int(round((buckets[i] - 1) * scaling))\n",
    "        new_obs = min(buckets[i] - 1, max(0, new_obs)) # VW: needed?\n",
    "        discretized.append(new_obs)\n",
    "    return tuple(discretized)\n",
    "\n",
    "def run_episode(env, agent):  \n",
    "    \"\"\"Runs the env for a certain amount of steps with the given parameters. Returns the reward obtained\"\"\"\n",
    "    obs = env.reset()\n",
    "    totalreward = 0\n",
    "    state=discretize_state(env, obs)\n",
    "    for _ in range(numTargetFrame):\n",
    "        state, reward, done = agent.train(state)   \n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward, agent.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestparams = None  \n",
    "bestreward = 0\n",
    "succeed = 0\n",
    "for i in range(numEpoch):  \n",
    "    agent=AgentRandomSearch(env)\n",
    "#     agent=AgentRandomSearch(env)\n",
    "#     gym.spaces.multi_discrete.MultiDiscrete(buckets)\n",
    "    \n",
    "    reward, parameters = run_episode(env, agent)\n",
    "    if reward > bestreward:\n",
    "        bestreward = reward\n",
    "        bestparams = parameters\n",
    "        # considered solved if the agent lasts for the required number of timesteps\n",
    "        if reward == numTargetFrame:\n",
    "            succeed = 1\n",
    "            break\n",
    "    if(i<10 or (i<100 and i%10==0) or (i<1000 and i%100==0) or (i%1000==0)):\n",
    "        print(\"Running epoch # {}...\".format(i))\n",
    "if (succeed==1):\n",
    "    print(\"Finished running and solution found in epoch # {}! =D \\n\".format(i)) # first epoch starts from label 0\n",
    "else:\n",
    "    print(\"Finished running but solution not found. =\\\\ \\n\")\n",
    "            \n",
    "print(\"#################################\")\n",
    "print(\"#                               #\")\n",
    "print(\"#        Done training!!        #\")\n",
    "print(\"#                               #\")\n",
    "print(\"#################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_episode(env, parameters):  \n",
    "    \"\"\" Records the frames of the environment obtained using the given parameters... Returns RGB frames\"\"\"\n",
    "    observation = env.reset()\n",
    "    firstframe = env.render(mode='rgb_array')\n",
    "    frames = [firstframe]\n",
    "    \n",
    "    for _ in range(numTargetFrame):\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        frames.append(frame)\n",
    "        if done:\n",
    "            break\n",
    "    return frames\n",
    "\n",
    "def display_frames_as_gif(frames, filename_gif = None):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 36)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50, repeat=False)\n",
    "    if filename_gif: \n",
    "        print(\"Saving animation...\")\n",
    "        anim.save(filename_gif, writer = 'pillow', fps=10)\n",
    "        print(\"Animation saves as gif at: {}\".format(filename_gif))\n",
    "        \n",
    "frames = show_episode(env, bestparams)\n",
    "display_frames_as_gif(frames, filename_gif=\"random_search_play.gif\")\n",
    "env.close()\n",
    "print(\"###############################\")\n",
    "print(\"#                             #\")\n",
    "print(\"#        Done saving!!        #\")\n",
    "print(\"#                             #\")\n",
    "print(\"###############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
